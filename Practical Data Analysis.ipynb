{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69520795-412f-4f72-ab88-b88cc40c4a9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0634a-cd7a-4176-8b10-8c7f6ef9d20f",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9407ca52-e850-4f74-890d-c6689520361b",
   "metadata": {},
   "source": [
    "One could think that the main object of data analysis is a set of observations $ \\{x_i\\} $ and an existing relationship between these measurements. These assumptions are mainly a belief that there is a deeper reality besides the measurement. For us, this relation is materialized by the probability distribution $P$.\n",
    "\n",
    "The probability of a measument $x$ given an a hypothesis $\\theta$ is $$P(x|\\theta).$$ There are two main methods of working in this space.\n",
    "\n",
    "* Parametric methods: We know the probability distribuition, or at least we think so, then we can simulate a sample;\n",
    "\n",
    "* Unparametric methods: We do not know the probability distribuition, therefore, we try to discover it via our observations set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4eb436-ced1-408b-bad4-f5426bbb00fe",
   "metadata": {},
   "source": [
    "## Dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9c637-01e0-4482-a973-cf379884d025",
   "metadata": {},
   "source": [
    "We begin by describing our dataset. For instance, say we have a set observation of the position of galaxies in our sky. If we start by applying statistics to their actual position, we would be inclined to a specific probability distribution $P$. Moreover, if we decide to \"clump\" our galaxies into particular pixels, we would find a whole new distribution.\n",
    "\n",
    "Furthermore, it is maximal that we correctly describe our data statistically. Let us say we have two different distributions:\n",
    "\n",
    "$$.\\;.\\;.\\;.\\;.\\;.\\;.\\;.\\;.\\;.\\;.\\; \\{x_1\\}$$\n",
    "\n",
    "$$........... \\{x_2\\}$$\n",
    "\n",
    "If we were to calculate the mean value of both, we would get the same value $\\overline{x}$, but it is clear that these dataset are very diferent. Therefore, we must calculate other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99407251-0f8f-4300-8ce0-569646d82076",
   "metadata": {},
   "source": [
    "## Computational methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffecae47-bb07-4e01-99a1-08fb1072e9d9",
   "metadata": {},
   "source": [
    "We shall use computational methods to generate a sample $\\{x_i\\}^N$ and calculate the value of $P(x)$ for every generated point. The first problem we will encounter is the fact that computers are intrinsically integer finite machines. \n",
    "\n",
    "The solution to our integrity problem lies in floating-point numbers. We shall give a tuple of an integer, and the position of the point. This is the definition of ***float values***!  (づ￣ ³￣)づ\n",
    "\n",
    "Now, to solve the finiteness problem, we implement interpolation. Intead of calculating every value $P(x)$, we will use two calculated values for $(x_1;P(x_1))$ and $(x_2; P(x_2))$ to find a third without the explicit calculation $P(x)$. As an example, we can find the value of $P(x)$ via a linear interpolation \n",
    "$$ \n",
    "\\overline{P}(x) = P_1 + (P_2 - P_1)\\frac{(x-x_1)}{(x_2-x_1)} \\;;\\; x_1<x<x_2\n",
    "$$\n",
    "\n",
    "Then, our fucntion will be described by\n",
    "\n",
    "$$\n",
    "P(x) = \\left\\{\n",
    "    \\begin{array}\\\\\n",
    "        P(x_1) & \\mbox{if } \\ x = x_1 \\\\\n",
    "        \\overline{P}(x) & \\mbox{if } \\ x = x \\\\\n",
    "        P(x_2) & \\mbox{if } \\ x=x_2 \\\\\n",
    "        \\vdots\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "We could use interpolation of a greater order. However, these methods are much more complex and require descriptions of the differential of $\\overline{P}(x)$  to impose continuity of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ddda22-a242-468f-871c-4035a9fa2fdd",
   "metadata": {},
   "source": [
    "It is also usual to think of the interpolated value $\\overline{P(x)}$ as the point in the linear equation $\\overline{P(x_i)} = a_i x + b_i$ with the following coeficients:\n",
    "$$\n",
    "a_i = \\frac{P(x_{i+1}) - P(x_i)}{x_{i+1} - x_i}\\;,\\;\\;b_i = P(x_i) - a_i x_i \\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2778c65-7cda-4451-aa00-7d3b4f918bb1",
   "metadata": {},
   "source": [
    "## Random varibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ef4ff-d331-46bd-802d-15f9418fafa3",
   "metadata": {},
   "source": [
    "Given a probability distribution $P(x)$, we define\n",
    "\n",
    "$$\n",
    "C(x) = \\int_a^b P(x) dx.\n",
    "$$\n",
    "\n",
    "Furthermore, let us say $x$ is a random variable, with an expected value\n",
    "\n",
    "$$\n",
    "\\langle x \\rangle = \\int P(x) x dx.\n",
    "$$\n",
    "\n",
    "Therefore, if $x$ is a random variable, then $C(x)$ will also be a random variable. Moreover, we might ask what is the probability of $C(x)$ be equal a certain value. This will give us a probability distribuition for $C$, \n",
    "\n",
    "$$\n",
    "P(C = C_0) = \\int P(x) \\delta(C(x) - C_0(x)) dx.\n",
    "$$\n",
    "\n",
    "This integral is easy to solve. By the definition of $C(x)$, we know $\\partial_x C(x) = P(x)$. Since the probability distribution is a positive definite function, $C(x)$ is a monotonic function, i.e., $C$ is an inversible function. Now, because $C$ is inversible, the argument of the Dirac's delta function is defined on only one point $C(x_0) = C_0$. Replacing $C_0(x)$ by $C(x_0)$ on the integral, and using the fact that\n",
    "\n",
    "$$\n",
    "\\delta(f(x)) = \\frac{\\delta(x-x_0)}{\\left|\\frac{\\partial f(x_0)}{\\partial x}\\right|},\n",
    "$$\n",
    "\n",
    "we get\n",
    "\n",
    "$$\n",
    "P(C = C_0) = \\frac{P(x_0)}{\\left|\\frac{\\partial C(x_0)}{\\partial x}\\right|} = 1.\n",
    "$$\n",
    "\n",
    "This confirms that our distribution gives the equal probability for every possible value of our dataset.\n",
    "\n",
    "Now we can calculate $C(x)$ using a set $\\{x_i , C_i\\}$. Not only $C(x)$, we can also calculate $\\widetilde{X}(C)$, using the fact that $C(x)$ is inversible. We are doing all this to get a random sample. Since a computer can give an algorithm that takes a random sample from a uniform distribution, we do not need to know how to choose values for the calculation of $P(x)$. Therefore, given an uniform distribution $U(a, b) \\rightarrow \\{U_i\\}$, we can generate a random sample ${X_i} = \\widetilde{X}(U_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381fe9e4-f6e0-4e55-a57d-d5ec307d006a",
   "metadata": {},
   "source": [
    "## Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6237fb0e-dfa8-4dba-b384-184f4d6e0b40",
   "metadata": {},
   "source": [
    "*For further reading, see* ***https://sites.warnercnr.colostate.edu/gwhite/wp-content/uploads/sites/73/2017/04/BinomialLikelihood.pdf*** \n",
    "\n",
    "Say we have a certain event with $P$ probability of happening. With $n$ occurencies, this particular event happended $r$ times, the likelihood will be given by\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(p|n, r) =\\binom{n}{r} p^r (1-p)^{n-r}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8633c1-70b2-418e-afac-95ae656e2ebd",
   "metadata": {},
   "source": [
    "# Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237414bd-977d-4302-bda4-95ab7ba3ebaa",
   "metadata": {},
   "source": [
    "## Describing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9090744c-d9cf-4bfa-add5-fd9164d43974",
   "metadata": {},
   "source": [
    "As we are working with *hard sciences*, most of our data will be quantitative. This is a reflection of our main objective, which is to build probability distribution from basic theoretical principles.\n",
    "\n",
    "We can divide quantitative data into two main fields: discrete and continuous.\n",
    "\n",
    "* Discrete: Probabilities are countable;\n",
    "\n",
    "* Continuous: Probabilities are uncountable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d09171a-1559-4fd0-99ad-f72eefbbdcba",
   "metadata": {},
   "source": [
    "## Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82a80d4-33d2-4b2e-b81a-ea4e7d65eaea",
   "metadata": {},
   "source": [
    "One is capable of building a map between discretes and continuos data with  histograms. For instance, given a colection of measurements of mass (*Data*)\n",
    "\n",
    "$$\n",
    "\\{m_{i}\\}_{i\\in\\left[1,N\\right]} = D\n",
    "$$\n",
    "\n",
    "we shall build a set knots\n",
    "\n",
    "$$\n",
    "\\{m_i^k\\}_{i\\in\\left[1, M+1\\right]},\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "b_i = Count\\{m_i^k \\leq m_i < m_{i+1}^k\\}.\n",
    "$$\n",
    "\n",
    "The set of $\\{b_i\\}_{i\\in\\left[1,M\\right]}$ is a discrete set. Furthermore, this set has a reduced dimensionality. This could mean we are losing information, but there are ways for us to measure how good our set of *bins* is.\n",
    "\n",
    "We call $b_i$ **statistics**. It is a function of our data, $b_i\\left(D\\right)$. Moreover, we will see differents examples of **statistics**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc293a0a-497c-4ca3-8516-cfa0eed5d624",
   "metadata": {},
   "source": [
    "### Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75a7bc-024f-4448-8e80-38154d158c6e",
   "metadata": {},
   "source": [
    "One possible sumary we can derive from our data is the *arithmetic mean*. It is defined as\n",
    "\n",
    "$$\n",
    "\\overline{X}(D) \\equiv \\sum_{i=1}^N \\frac{X_i}{N}\\;.\n",
    "$$\n",
    "\n",
    "Additionaly, we can calculate the *mean* from function with\n",
    "\n",
    "$$\n",
    "\\overline{f}(D) \\equiv \\sum_{i=1}^N \\frac{f(x_i)}{N}\\;.\n",
    "$$\n",
    "\n",
    "There are different *mean values*. For instance, we can summarize our data into the *geometric mean* with\n",
    "\n",
    "$$\n",
    "\\overline{X}^g (D)\\equiv \\prod_{i=1}^N \\left[X_i\\right]^{\\frac{1}{N}},\n",
    "$$\n",
    "\n",
    "which could usefully be written as\n",
    "\n",
    "$$\n",
    "\\overline{X}^g (D) = e^{\\sum_{i=1}^N\\ln X_i/N}.\n",
    "$$\n",
    "\n",
    "There are two other *mean* values. *Harmonic mean* and *root mean square*, respectively given as\n",
    "\n",
    "$$\n",
    "H \\equiv \\frac{N}{\\sum_{i=1}^N \\frac{1}{X_i}},\n",
    "$$\n",
    "\n",
    "$$\n",
    "RMS \\equiv \\sqrt{\\frac{\\sum_{i=1}^N X_i^2}{N}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc63b904-d444-425a-97bd-e465bbafac50",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff36722-d868-45f9-809f-2bef3c3cc47c",
   "metadata": {},
   "source": [
    "Given three measurements,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9063b0a-52b6-41ef-98f0-4980a59d6f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf89e900-2ab9-4840-a3a3-0d9aef02a1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: 1\n",
      "x2: 2.718281828459045\n",
      "x3: 148.4131591025766\n"
     ]
    }
   ],
   "source": [
    "x1 = 1\n",
    "x2 = np.e\n",
    "x3 = np.exp(5)\n",
    "arr = np.array([x1, x2, x3])\n",
    "\n",
    "print(f'x1: {x1}\\nx2: {x2}\\nx3: {x3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f0133-8501-4aa5-b64f-566e1b3a2bc2",
   "metadata": {},
   "source": [
    "if we calculate the *arithmetic mean*, $\\overline{X}$, we will find a value dominated by the biggest measurement, $e^5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d3bde5-224e-438a-b9aa-ad81cca15783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_bar: 50.71048031034522\n"
     ]
    }
   ],
   "source": [
    "x_bar = np.mean(arr)\n",
    "\n",
    "print(f'x_bar: {x_bar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf79081a-f0f4-4d24-943d-ace843bd51f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(arr, len(arr) * [1], 'x')\n",
    "plt.plot(x_bar, 1, 'o', label = 'x_bar')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e72aba9-c200-4bfd-95a1-242ce446f58d",
   "metadata": {},
   "source": [
    "As we can see, the value of $\\overline{X}$ is masked by the dispersion of this distribution. However, if we calculate $\\overline{X}^g$, we find a reasonable description of our measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b9c8f7-c95c-48a9-83bc-59771109853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ca7534c-9811-43c3-ae70-958207a7223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_barg: 7.38905609893065\n"
     ]
    }
   ],
   "source": [
    "x_barg = gmean(arr)\n",
    "\n",
    "print(f'x_barg: {x_barg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "251918a6-075b-42fb-98ce-7ef406f3ffee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(arr, len(arr) * [1], 'x')\n",
    "plt.plot(x_barg, 1, 'o', label = 'x_barg')\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8283df35-ee4b-40c9-bef1-8e0d65d9505a",
   "metadata": {},
   "source": [
    "### Median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28229333-f408-4102-944c-19cf2622ebfe",
   "metadata": {},
   "source": [
    "The *median* is the mean value of the midle terms from the ordenation of out sample.\n",
    "\n",
    "Let us say that \n",
    "$$\n",
    "Y_i = X_{\\mathcal O(i)},\\; where \\; Y_{i} \\leq Y_{i+1}.\n",
    "$$ \n",
    "\n",
    "The *median* is $Y_{|\\underline{N/2}|}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e1e828-12bc-4e17-a6b5-7a88747cacaf",
   "metadata": {},
   "source": [
    "### Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5b7796-a867-4d32-a8d2-bebb175d21bf",
   "metadata": {},
   "source": [
    "*Mode* is the value with highest repetition from a sample.\n",
    "\n",
    "$$\n",
    "L_i = X_i,\\; where \\; Count\\{X_i\\}>Count\\{X_j\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4c296-82a2-4486-b480-81a3175d0ace",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Robust Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7e8c0-1adf-4e71-85e9-9044f975f3bc",
   "metadata": {},
   "source": [
    "**Robust statistics** is statistics with good performance for data drawn from a wide range of probability distributions, especially for distributions that are not normal. Robust statistical methods have been developed for many common problems, such as estimating location, scale, and regression parameters. One motivation is to produce statistical methods that are not unduly affected by outliers. [[Wiki]](https://en.wikipedia.org/wiki/Robust_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceead0f5-a65d-41ef-a4e0-890e40e9fef5",
   "metadata": {},
   "source": [
    "### Mean Absolute Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8b4e82-bea8-4b10-8588-62d0172dc481",
   "metadata": {},
   "source": [
    "The mean absolute deviation of a dataset is the average distance between each data point and the mean. It gives us an idea about the variability in a dataset.[[Khan]](https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/other-measures-of-spread/a/mean-absolute-deviation-mad-review)\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N\\frac{\\left|X_i - \\overline{X}\\right|}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288549be-1343-455d-adec-a4552d9b1aa0",
   "metadata": {},
   "source": [
    "### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65adf92d-f337-4a63-8a31-2e4080a6cd1a",
   "metadata": {},
   "source": [
    "Variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value.\n",
    "\n",
    "$$\n",
    "V(X) \\equiv \\frac{1}{N} \\sum_{i=1}^N \\left(X_i - \\overline{X}\\right)^2 = \\overline{X^2} - \\overline{X}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d52335-afef-4db2-a40e-2ebb5a44af61",
   "metadata": {},
   "source": [
    "### Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c333d-fc1b-47ac-a174-75c0b1eb5cad",
   "metadata": {},
   "source": [
    "The standard deviation is a measure of the amount of variation or dispersion of a set of values.\n",
    "\n",
    "$$\n",
    "\\sigma \\equiv \\sqrt{V(X)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894fbb3c-b492-4d21-8bbf-4ab7eff9013e",
   "metadata": {},
   "source": [
    "### Skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c1258-a826-45ee-8272-a81aabb58a38",
   "metadata": {},
   "source": [
    "Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined.\n",
    "\n",
    "$$\n",
    "\\gamma \\equiv \\frac{1}{N \\sigma^3}\\sum_{i=1}^N \\left(X_i - \\overline{X}\\right)^3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1904901d-ab0d-4ecb-8dfc-70d16a891ebd",
   "metadata": {},
   "source": [
    "### Kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5221828-fbfb-4f39-a3f7-1c6d9a38f129",
   "metadata": {},
   "source": [
    "Kurtosis (from Greek: κυρτός, kyrtos or kurtos, meaning \"curved, arching\") is a measure of the \"tailedness\" of the probability distribution.\n",
    "\n",
    "$$\n",
    "K \\equiv \\frac{1}{N \\sigma^4}\\sum_{i=1}^N \\left(X_i - \\overline{X}\\right)^4 - 3\n",
    "$$\n",
    "\n",
    "This extremely weird $-3$ is a normalization factor so that the *kurtosis* of the *Gaussian Distribuition* is **zero**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a2ad7f-afce-448e-b544-70e6ecc1b713",
   "metadata": {},
   "source": [
    "### Central Moment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a3c7e-9b3d-44d6-bcd5-221e94c3e60e",
   "metadata": {},
   "source": [
    "Given a estimator\n",
    "\n",
    "$$\n",
    "M_r \\equiv \\frac{1}{N} \\sum_{i=1}^N \\left(X_i - \\overline{X}\\right)^r = \\overline{\\left(X_i - \\overline{X}\\right)^r},\n",
    "$$\n",
    "\n",
    "called *central moment*, we are could describe a new summary\n",
    "\n",
    "$$\n",
    "\\{\\overline{X}, M_2, M_3, ..., M_N\\}.\n",
    "$$\n",
    "\n",
    "This new summary is capable of correctly define all our data's information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a073801-b67c-4705-95ab-0088d1da3314",
   "metadata": {},
   "source": [
    "## Multivariable Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cbbf8a-7745-4f73-b5b2-e93e716fae8a",
   "metadata": {},
   "source": [
    "Sometimes our measurements give rise to multiple information, *i.e.*,\n",
    "\n",
    "$$\n",
    "D = \\{\\left(X_i, Y_i)\\right)\\}_{i \\in \\left[1, N\\right]}.\n",
    "$$\n",
    "\n",
    "In this case, we need to apply different **statistics** to summarize this information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3c6a05-63dc-4962-8ab6-90e512565599",
   "metadata": {},
   "source": [
    "### Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d870116-7ca6-4265-adbb-a9c48fcb7005",
   "metadata": {
    "tags": []
   },
   "source": [
    "Covariance is a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values (that is, the variables tend to show similar behavior), the covariance is positive. \n",
    "\n",
    "In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, (that is, the variables tend to show opposite behavior), the covariance is negative. The sign of the covariance therefore shows the tendency in the linear relationship between the variables. [[Wiki]](https://en.wikipedia.org/wiki/Covariance)\n",
    "\n",
    "$$\n",
    "Cov\\left(X,Y\\right) \\equiv \\frac{1}{N} \\sum_{i=1}^N \\left(X_i - \\overline{X}\\right)\\left(Y_i - \\overline{Y}\\right)\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5058bfa4-731b-427d-af82-4e79de9e6da8",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ab140-a4c0-4156-9b21-e8088ecc94de",
   "metadata": {
    "tags": []
   },
   "source": [
    "Correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data. Although in the broadest sense, \"correlation\" may indicate any type of association, in statistics it normally refers to the degree to which a pair of variables are linearly related.[[Wiki]](https://en.wikipedia.org/wiki/Correlation)\n",
    "\n",
    "Correlation is a normalized way of writing the covariance.\n",
    "\n",
    "$$\n",
    "Cor\\left(X, Y\\right) = \\rho_{XY} \\equiv \\frac{Cov\\left(X, Y\\right)}{\\sigma_X \\sigma_Y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a24a05-0fb4-419b-a023-2e6f0d987289",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lesson 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412832e8-ae0a-4a73-b7f6-bea391c2f083",
   "metadata": {},
   "source": [
    "## Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a804d26a-37d4-41ac-8d94-ed9790cae0b2",
   "metadata": {},
   "source": [
    "Let's say we have four identical coins. They have two sides - heads or tails - with an equal probability of arising. Taking a deeper look into the probabilities, we can build our distribution. They are the following\n",
    "\n",
    "* Four tails: $\\frac{1}{16}$\n",
    "* Three tails: $\\frac{4}{16}$\n",
    "* Two tails: $\\frac{6}{16}$\n",
    "* One tail: $\\frac{4}{16}$\n",
    "* Four heads: $\\frac{1}{16}$\n",
    "\n",
    "Therefore, it is expected that if we toss our set of coins sixteen times, we should see this distribution. Let us create a simple example with a numerical analogy. In the next kernel, we will create an array with zeroes (tails) and ones (heads). You will probably see that is very unlikely our distribution matches with our random experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11084a08-2c9a-4d83-9596-977031eae80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1],\n",
       "       [0, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 0, 1, 0],\n",
       "       [0, 1, 1, 1],\n",
       "       [0, 1, 1, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [1, 0, 1, 0],\n",
       "       [1, 1, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 1, 0, 1],\n",
       "       [0, 1, 1, 1],\n",
       "       [1, 1, 0, 0],\n",
       "       [1, 0, 1, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creates a numpy array of shape (16,4), filled with random integers between 0 (inclusive) and 2 (exclusive)\n",
    "rand_array = np.random.randint(0, 2, (16,4)) \n",
    "rand_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79cabde-d4f5-4397-8921-99f0e0b4b630",
   "metadata": {},
   "source": [
    "## Law of Large Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f3f30-e952-4b53-9689-887242863af2",
   "metadata": {},
   "source": [
    "The **Law of Large Numbers** is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the expected value and tends to become closer to the expected value as more trials are performed.[[Wiki]](https://en.wikipedia.org/wiki/Law_of_large_numbers) Furthermore, as our sample grows, the empirical probability distribution narrows down to the theoretical distribution, *i.e.*\n",
    "\n",
    "$$\n",
    "\\frac{n_i}{N} \\rightarrow P_i \\;| N\\rightarrow \\infty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8285b4-7997-4607-b6ef-2916d66c6979",
   "metadata": {},
   "source": [
    "## Expected Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a56b1-a5c8-4c6b-bfa2-8e487de41c78",
   "metadata": {},
   "source": [
    "**Expected value** (also called expectation, expectancy, mathematical expectation, mean, average, or first moment) is a generalization of the weighted average. Informally, the expected value is the arithmetic mean of a large number of independently selected outcomes of a random variable.[[Wiki]](https://en.wikipedia.org/wiki/Expected_value)\n",
    "\n",
    "$$\n",
    "\\langle r\\rangle  = \\sum_r r P(r)\\; .\n",
    "$$\n",
    "\n",
    "Moreover, one can find the expected value of a function with:\n",
    "\n",
    "$$\n",
    "\\langle f\\rangle  = \\sum_r f(r) P(r)\\; .\n",
    "$$\n",
    "\n",
    "It is easy to show that\n",
    "\n",
    "$$\n",
    "\\langle f+g\\rangle  = \\sum_r \\left(f(r)+g(r)\\right) P(r) = \\langle f\\rangle  + \\langle g\\rangle \\;,\n",
    "$$\n",
    "\n",
    "and if $P(r,s) = P_1(r)P_2(s)$, $f(r,s) = f(r)$ and $g(r,s) = g(s)$\n",
    "\n",
    "$$\n",
    "\\langle fg\\rangle  = \\sum_{r,s} f(r)g(s) P_1(r)P_2(s) = \\langle f\\rangle \\langle g\\rangle \\;.\n",
    "$$\n",
    "\n",
    "Let us roll back to our last example. What is the expected value of the number of tails?? Using the probability distribution we get:\n",
    "\n",
    "$$\n",
    "\\langle r\\rangle  = 0 \\times \\frac{1}{16} + 1 \\times \\frac{4}{16} + 2 \\times \\frac{6}{16} + 3 \\times \\frac{4}{16} + 4 \\times \\frac{1}{16} = \\frac{32}{16} = 2   \n",
    "$$\n",
    "\n",
    "The most probable number of tails is two, and this seems like a reasonable answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf93c3-6d04-42ed-9969-df7099c2164f",
   "metadata": {},
   "source": [
    "## Probability Density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0c4e93-bfa8-44ed-9ce0-11b9b9c384a2",
   "metadata": {},
   "source": [
    "What if our random varible takes values from the $\\mathcal{R}^1$? For instance, if the measurement of a certain ruller is $11 cm$, the probability of this value is\n",
    "\n",
    "$$\n",
    "\\frac{n_{11cm}}{N} = P_{11cm}\\;.\n",
    "$$\n",
    "\n",
    "However, since $N = \\infty$, the probability for every possible value in the number line is $zero$. To counter this theoretical *conundrum*, we will create sections of our possible values. Given a sufficiently small $\\varepsilon$, our new probability shall be written as\n",
    "\n",
    "$$\n",
    "\\frac{n\\left[(11cm - \\varepsilon, 11cm + \\varepsilon)\\right]}{N} = P_{interval}\\;.\n",
    "$$\n",
    "\n",
    "With this in mind, we will generaly redefine probability on a said interval $P[(a,b)] \\equiv P(x)$.  Now we define $P'(x) = p(x)$, *i.e.*\n",
    "\n",
    "$$\n",
    "P(x) = \\int p(x) dx\\;,\n",
    "$$\n",
    "where $p(x)$ is the **Probability density**.\n",
    "\n",
    "\n",
    "The **Probability density**, or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would be close to that sample. In other words, while the absolute likelihood for a continuous random variable to take on any particular value is 0 (since there is an infinite set of possible values to begin with), the value of the PDF at two different samples can be used to infer, in any particular draw of the random variable, how much more likely it is that the random variable would be close to one sample compared to the other sample. [[Wiki]](https://en.wikipedia.org/wiki/Probability_density_function)\n",
    "\n",
    "As well as we ahve done for discrete values, we can define expected values with the **probability density**. \n",
    "\n",
    "$$\n",
    "\\langle x\\rangle  = \\int_{-\\infty}{^\\infty} x p(x) dx\\;;\\\\\n",
    "\\langle f\\rangle  = \\int_{-\\infty}{^\\infty} f(x) p(x) dx\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff949e-927d-4e09-b3ad-cf7f9e37c1b9",
   "metadata": {},
   "source": [
    "## Binomial Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20674953-3d4b-4798-91c2-5a6796d2dd72",
   "metadata": {},
   "source": [
    "Given a experiment, with two -and only two- possible outcomes $\\{A, B\\}$, the sum of probabilities $P_A$ and $P_B$ will be equal to $one$. Additionally, with $N$ events, the probability of only $p$ measurements being $A$ is\n",
    "$$\n",
    "P(p) = P_A^{p}P_B^{N-p}\\binom{N}{p}\\;.\n",
    "$$\n",
    "\n",
    "It is easy to check the normalization of our probability. If we sum the above expression for every possible $p$ we find\n",
    "$$\n",
    "\\sum_p^N P(p) = \\sum_p^N P_A^{p}P_B^{N-p}\\binom{N}{p}\\;.\n",
    "$$\n",
    "Where the right side is know as the binomial expansion [[Wiki]](https://en.wikipedia.org/wiki/Binomial_theorem), therefore\n",
    "$$\n",
    "\\sum_p^N P(p) = (P_A + P_B)^N = 1\\;.\n",
    "$$\n",
    "\n",
    "Moreover, we can calculate the expected value with a simple trick. Since $\\langle p\\rangle  = \\sum_{p=0}^N p P(p)$, we will replace the probability of $p$ by its binomial form. The trick is to notice the value of $p$ in the expoent and as a factor of the sum. Then we shall write the binomial form times the value $p$ as a derivative.\n",
    "\n",
    "$$\n",
    "\\langle p\\rangle  = \\sum_{p=0}^N p P_A^{p}P_B^{N-p}\\binom{N}{p} = P_a \\left[ \\frac{\\partial}{\\partial P_A} \\left( \\sum_{p=0}^N P_A^p P_B^{N-p} \\binom{N}{p} \\right) \\right] = P_A \\frac{\\partial}{\\partial P_A}\\left(P_A + P_B\\right)^N\\;,\\\\\\langle p\\rangle  = N P_A\n",
    "$$\n",
    "\n",
    "Furthermore, one can find the variance of the distribuition. We know the variance of a value is $\\langle \\alpha^2\\rangle  - \\langle \\alpha\\rangle ^2$. Using the derivation trick twice on the binomial form of $P(p)$ we find\n",
    "\n",
    "$$\n",
    "\\langle p (p -1)\\rangle  = P_A^2 \\frac{\\partial}{\\partial P_A}\\left(P_A + P_B\\right)^N = P_A^2N(N-1)\\;.\n",
    "$$\n",
    "\n",
    "Using the sum property for the expected value, it is clear that $\\langle p^2\\rangle  = P_A^2N(N-1) + \\langle p\\rangle $. Therefore, the variance will be\n",
    "\n",
    "$$\n",
    "\\langle p^2\\rangle  - \\langle p\\rangle ^2 = N P_A P_B\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b8774d-fc7c-4fad-94a6-1ca9f9728022",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lesson 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4922b52d-e6cc-44ff-a60a-f118138b8454",
   "metadata": {},
   "source": [
    "## Poisson's Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24621e42-efc9-443d-a866-ffa473b0bec9",
   "metadata": {},
   "source": [
    "Suppose we have an average of $\\lambda$ events that will happen in a given continuous interval $(a,b)$. Additionally, we subdivide our interval into $n$ equally sparsed sections. We begin by questioning what is the probability of $r$ events occurring in one particular section $n_x$.\n",
    "$$\n",
    "_a(- - -\\;r- - - - - - - - - -)_b\n",
    "$$\n",
    "\n",
    "Based on the **Binomial distribution**, we shall fix the expected value $\\langle r\\rangle  = n P = \\lambda$ as $n$ tends to $\\infty$. With $P_B = 1-P_A$ and $P_A = \\lambda / n$, we find:\n",
    "\n",
    "$$\n",
    "P(r) = \\left(\\frac{\\lambda}{n}\\right)^r \\left(1 - \\frac{\\lambda}{n}\\right)^{n-r} \\binom{n}{r}\\;.\n",
    "$$\n",
    "\n",
    "Moreover, using **Stirling approximation** we can reduce the above expression to:\n",
    "\n",
    "$$\n",
    "P(r) = e^{-\\lambda}\\frac{\\lambda^r}{r!}\\;.\n",
    "$$\n",
    "\n",
    "One can also prove that a poissonic distribution has variance of $\\lambda$ and a standard deviation of $\\sigma_r = \\sqrt{\\lambda}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac36fb31-b73b-492d-87a9-148b479d1e85",
   "metadata": {},
   "source": [
    "## Gaussian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c69fe-0886-4c73-86dd-385e7be85980",
   "metadata": {},
   "source": [
    "The Gaussian distribution is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x- \\mu)^2}{2 \\sigma^2}}\\;.\n",
    "$$\n",
    "\n",
    "With a couple of tricks we can show that:\n",
    "\n",
    "- The distribution is normalized, *i.e.*, $\\int_{-\\infty}^{\\infty}f(x)dx = \\int_{-\\infty}^{\\infty}dx\\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x- \\mu)^2}{2 \\sigma^2}} = 1\\;;$\n",
    "- Its expected value $\\langle x\\rangle  = \\int_{-\\infty}^{\\infty}x \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x- \\mu)^2}{2 \\sigma^2}}dx  = \\mu \\;;$\n",
    "- Its variance is $\\langle (x-\\mu)^2\\rangle  = \\int_{-\\infty}^{\\infty}dx (x-\\mu)^2 \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{(x- \\mu)^2}{2 \\sigma^2}} = \\sigma^2 \\;.$\n",
    "\n",
    "***P.S.:*** *This derivation would be a pain to do entirely in $\\LaTeX$. There is a Xournal File in the repository with the complete analysis.* \n",
    "\n",
    "Additionally, we shall formally call $1\\sigma$ the calculated area bellow the normal distribution between the interval $(\\mu-\\sigma, \\mu+\\sigma)$. This is an important value because it does not depend on $\\sigma$ and $\\mu$! By calculating the integral on a said interval, with a simple variable change we find:\n",
    "\n",
    "$$\n",
    "1\\sigma = \\int_1^1 \\frac{e^{-\\frac{y^2}{2}}}{\\sqrt{2\\pi}}dy \\approx 0,6827...\n",
    "$$\n",
    "\n",
    "It is common to say that a certain event has a probability $1\\sigma$ of happening. This is widely used as a range of confiability. As we get more $\\sigma$, there will be a greater confiability of a certain expected value:\n",
    "\n",
    "- $1\\sigma = 0,6827...$\n",
    "- $1,645\\sigma = 0,9$\n",
    "- $2\\sigma = 0,9545...$\n",
    "- $3\\sigma = 0,9973...$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cbb528-a1ba-4d54-a977-e719379e8485",
   "metadata": {},
   "source": [
    "## Multivariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96753bd-e888-4e87-ae50-d751f31ca8db",
   "metadata": {},
   "source": [
    "Given a vector $\\vec{x} \\in \\mathcal{M}^n$, the probability of measuring a certain value for $\\vec{x}$ is:\n",
    "\n",
    "$$\n",
    "P(\\vec{x}) = \\frac{\\exp\\left[{\\frac{- (\\vec{x} - \\vec{\\mu})^T C^{-1} (\\vec{x}-\\vec{\\mu})}{2}}\\right]}{\\left(\\sqrt{2\\pi}\\right)^n \\sqrt{det\\;C}}\\;.\n",
    "$$\n",
    "\n",
    "Where $C$ is a defined-positive symmetric matrix, *i.e.*, $\\frac{\\vec{v}^T C \\vec{v}}{|\\vec{v}|} > 0$ for every $\\vec{v} \\in \\{v\\}$ (a possible basis for $\\mathcal{M}^n$). Furthermore, the *eigen-value* of $C$ is always:\n",
    "\n",
    "$$\n",
    "C\\vec{v_i} = (\\sigma^i)^2\\vec{v_i}\\;.\n",
    "$$\n",
    "\n",
    "Now, we shall prove that this distribution is normalized. The integral we need to evaluate is:\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty}P(\\vec{x})d^n x = \\int_{-\\infty}^{\\infty} \\frac{\\exp\\left[{\\frac{- (\\vec{x} - \\vec{\\mu})^T C^{-1} (\\vec{x}-\\vec{\\mu})}{2}}\\right]}{\\left(\\sqrt{2\\pi}\\right)^n \\sqrt{det\\;C}} d^n x\\;.\n",
    "$$\n",
    "\n",
    "Let us call $\\vec{x}-\\vec{\\mu} = \\alpha^i\\vec{v_i}$, therefore the derivative on ints components is $\\frac{\\partial x^i}{\\partial \\alpha^j} = v^i_j$. From the *eigen-value* equation, we find that $C^{-1}(\\alpha^i\\vec{ v_i}) =\\sum_i \\frac{1}{(\\sigma^i)^2} (\\alpha^i \\vec{v_i})$. Furthermore, knowing that our basis $\\{v\\}$ is orthogonal means $\\vec{v_i} \\cdot \\vec{v_j} = \\delta_{ij}$, we can rewrite our integral as:\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty}P(\\vec{x})d^n x = \\int_{-\\infty}^{\\infty} \\frac{e^{-\\sum_i \\frac{(\\alpha^i)^2}{2(\\sigma^i)^2}}}{(\\sqrt{2\\pi})^n \\prod_i (\\sigma^i)} d^n \\alpha\\;.\n",
    "$$\n",
    "\n",
    "Where $\\prod_i (\\sigma^i) = \\sqrt{det\\;C}$, by diagonalizing the matrix in its *eigen-values*. One can see that for every $n$ possible index, $i$, we will find an integral with known value of $1$.\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty} \\frac{e^{-\\frac{(\\alpha^1)^2}{2(\\sigma^1)^2}}}{\\sqrt{2\\pi} (\\sigma^1)} d \\alpha^1 \\times \\int_{-\\infty}^{\\infty} \\frac{e^{-\\frac{(\\alpha^2)^2}{2(\\sigma^2)^2}}}{\\sqrt{2\\pi} (\\sigma^2)} d \\alpha^2 \\times \\int_{-\\infty}^{\\infty} \\frac{e^{-\\frac{(\\alpha^3)^2}{2(\\sigma^3)^2}}}{\\sqrt{2\\pi} (\\sigma^3)} d \\alpha^3 \\times ... \\times \\int_{-\\infty}^{\\infty} \\frac{e^{-\\frac{(\\alpha^n)^2}{2(\\sigma^n)^2}}}{\\sqrt{2\\pi} (\\sigma^n)} d \\alpha^n = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801736fd-1433-4240-afe3-2304b47c3de7",
   "metadata": {},
   "source": [
    "## Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3617b5-c846-49de-9a18-f2e51234815c",
   "metadata": {},
   "source": [
    "Suppose we have a random variable $Z$ given by the sum of random variables $x_i$, where $x_i$ are not identically independently distributed (i.i.d). The expected value of $Z$ is\n",
    "$$\n",
    "\\langle Z\\rangle  = \\sum_{i=1}^N\\langle x_i\\rangle  = \\sum_{i=1}^N \\int d^n x P(x) x_i = \\sum_{i=1}^N \\mu_i\\;,\n",
    "$$\n",
    "where $\\mu_i$ is the mean value of $x_i$. The variance of $Z$ is\n",
    "$$\n",
    "V(Z) = \\langle(Z - \\langle Z \\rangle)^2\\rangle = \\bigg \\langle \\left[\\sum_{i=1}^N (x_i - \\mu_i)\\right]^2 \\bigg \\rangle = \\bigg \\langle \\sum_{i=1}^N (x_i - \\mu_i) \\sum_{j=1}^N (x_j - \\mu_j) \\bigg \\rangle\\;,\n",
    "$$\n",
    "which can be divided into two expected values, one was $i=j$ and one with $i\\ne j$. By doing this we find\n",
    "$$\n",
    "V(Z) = \\bigg \\langle \\sum_{i=1}^N (x_i - \\mu_i)^2 \\bigg \\rangle + \\bigg \\langle \\sum_{i \\ne j}^N (x_i - \\mu_i)(x_j - \\mu_j) \\bigg \\rangle = V_i + C_{ij}\\;,\n",
    "$$\n",
    "where $V_i$ is the variance of every $x_i$ and $C_{ij}$ is the estimator of the covariance.\n",
    "\n",
    "Reviewing our variable $Z$, we see that it has an expected value and variance given by the respective sum of its components, *i.e.*\n",
    "\n",
    "- $\\mu_Z = \\sum \\mu_i$;\n",
    "- $V(Z) = \\sum V_i$.\n",
    "\n",
    "This information tells us that these values shall asymptotically ascend to a Gaussian distribution as $N$ tends to $\\infty$.\n",
    "\n",
    "The **Central limit theorem** establishes that, in many situations, when independent random variables are summed up, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed.[[Wiki]](https://en.wikipedia.org/wiki/Central_limit_theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8416d8-66df-407c-a51d-c1baed2a9495",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lesson 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8714f8-db6e-4c29-ad77-8fac7cdf9855",
   "metadata": {},
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aadb99-c441-4af1-a7b5-125b5b7b9239",
   "metadata": {},
   "source": [
    "Given a certain sample $\\{x_i\\}$ related to a particular distribution $P(\\{x_i\\})$, we say $f(\\{x_i\\})$ is a consistent estimator of $\\theta$ -being $\\theta$ a particular property of $P$- if $\\lim_{N \\to \\infty} f(\\{x_i\\})= \\theta$. Furthermore, $f(\\{x_i\\})$ will not be biased when $\\langle f(\\{x_i\\}) \\rangle = \\theta$. Finnally, $f(\\{x_i\\})$ will be efficient if its variance is sufficiently small. A function $f(\\{x_i\\})$ that respects these three caracteristics is called an estimator. **VAGUE ಠ_ಠ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bb8d9d-acfb-43a6-9451-66e93390c4aa",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4234ddde-4bd8-4728-bf3a-e05a6b1085ff",
   "metadata": {},
   "source": [
    "We already know the expected value of a distribution, $\\mu = \\langle x \\rangle = \\int x P(x) dx$. Based on the average value, we shall create a bunch of estimators.\n",
    "\n",
    "* $\\hat{\\mu}_1 = \\sum_{i=1}^N \\frac{x_i}{N}$: Consistent, unbiased, and efficienty;\n",
    "\n",
    "* $\\hat{\\mu}_2 = \\sum_{i=1}^{Min(N, 10)} \\frac{x_i}{Min(N, 10)}$: Unconsisten, unbiased, less efficient than $\\hat{\\mu}_1$ for $N>10$;\n",
    "\n",
    "* $\\hat{\\mu}_3 = \\sum_{i=1}^N \\frac{x_i}{N-1}$: Consistent, biased, slightly less efficient than $\\hat{\\mu}_1$;\n",
    "\n",
    "* $\\hat{\\mu}_4 = 1,8$: Consistent and unbiased if and only if $\\mu = 1,8$, but infinitedly efficient;\n",
    "\n",
    "* $\\hat{\\mu}_5 = \\left( \\prod_{i=1}^N x_i\\right)^\\frac{1}{N}$: Consistancy and efficiency depends on $P(x)$, bias depends on the expected value of $x$;\n",
    "\n",
    "* $\\hat{\\mu}_6 = x_p$, where $x_p$ is the most frequent value of $\\{x_i\\}$: Consistancy, bias and efficiency depends on $P(x)$;\n",
    "\n",
    "* $\\hat{\\mu}_7 = \\frac{Max(\\{x_i\\}) + Min(\\{x_i\\})}{2}$: Consistancy, bias and efficiency depends on $P(x)$;\n",
    "\n",
    "* $\\hat{\\mu}_8 = \\sum_{i=1}^{|\\underline{N/2}|} \\frac{x_{2i}}{|\\underline{N/2}|}$, where $|\\underline{N/2}| = \\frac{N}{2}$ for even $N$, and $|\\underline{N/2}| = \\frac{(N-1)}{2}$ for odd values: Consistent, unbiased, less efficient than $\\hat{\\mu}_1$.\n",
    "\n",
    "We fall on the assumption that the first estimator is the best, but that is not true for every sample. As an exercise, you should check the consistency, efficiency, and bias of every estimator above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33620b93-c80a-403b-9be3-f4f0afba6f58",
   "metadata": {},
   "source": [
    "## Likelihood and Fisher's Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7237e128-c4ef-4985-9a67-cd8a6a8a298d",
   "metadata": {},
   "source": [
    "The likelihood function (often simply called the likelihood) describes the joint probability of the observed data as a function of the parameters of the chosen statistical model. For each specific parameter value $\\theta$  in the parameter space, the likelihood function $P(X, \\theta)$ therefore assigns a probabilistic prediction to the observed data $X$. Mathematically, we say that given an hypothesis $H_0$, the likelihood is\n",
    "$$\n",
    "\\mathcal{L}\\left(\\{x_i\\}|H_0\\right) = \\prod_{i=1}^N P(x_i|\\theta)\\;.\n",
    "$$\n",
    "The hypothesis encapsulates the functional form of $P(x_i)$ and the set of parameters $\\{\\theta\\}$.\n",
    "\n",
    "In terms of the likelihood, the expected value of a statistics is given by\n",
    "$$\n",
    "\\langle f \\rangle = \\int d^n x f(\\{x_i\\}) \\mathcal{L}\\left(\\{x_i\\}|\\vec{\\theta}\\right)\\;.\n",
    "$$\n",
    "From now on, we will write $\\mathcal{L}\\left(\\{x_i\\}|\\vec{\\theta}\\right)$ as $\\mathcal{L}$. Now, say we have an unbiased estimator $\\hat{\\theta}(\\{x_i\\})$, then\n",
    "$$\n",
    "\\langle \\hat{\\theta} \\rangle = \\int d^n x \\hat{\\theta}(\\{x_i\\}) \\mathcal{L} = \\theta \\;,\n",
    "$$\n",
    "which is a simple function of $\\theta$. Therefore, we can differentiate the right equation. By doing this we find\n",
    "$$\n",
    "\\int d^n x \\hat{\\theta}(\\{x_i\\}) \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = 1\\;.\n",
    "$$\n",
    "Dividing both sides by $\\mathcal{L}$, and rewriting $\\frac{1}{\\mathcal{L}}\\frac{\\partial \\mathcal{L}}{\\partial \\theta}$ as $\\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta}$ we get\n",
    "$$\n",
    "\\int d^n x \\hat{\\theta}(\\{x_i\\}) \\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta} \\mathcal{L} = 1\\;.\n",
    "$$\n",
    "\n",
    "Beyond that, we know that the distribution is normalizes, *i.e*,\n",
    "$$\n",
    "\\int d^n x \\mathcal{L} = 1\\;,\n",
    "$$\n",
    "were its derivative is\n",
    "$$\n",
    "\\int d^n x \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = 0\\; ;\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\int  \\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta} \\mathcal{L} d^n x = 0\\;.\n",
    "$$\n",
    "\n",
    "This last equation can be seen as the expected value of $\\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta}$, therefore, this expression is a statistic with null expected value,\n",
    "$$\n",
    "\\bigg \\langle \\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta} \\bigg \\rangle = 0\\;.\n",
    "$$\n",
    "\n",
    "Finnally, we subtract $\\theta$ times the null integral from the unitary integral, *i.e*\n",
    "$$\n",
    "\\int d^n x \\left[\\hat{\\theta}(\\{x_i\\}) - \\theta \\right] \\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta} \\mathcal{L} = 1\\;.\n",
    "$$\n",
    "\n",
    "Before we go on, we must remind a little about the **Cauchy-Schwarz inequality**. We will use a possible generalization, when we define the norm of a function as\n",
    "$$\n",
    "\\int f^*f d^n x = \\int f^2 d^n x\\;.\n",
    "$$\n",
    "Then, the inequality becomes\n",
    "$$\n",
    "\\left(\\int f^2 d^n x\\right)\\left(\\int g^2 d^n x\\right)\\geq \\left(\\int fg d^n x\\right)^2\\;.\n",
    "$$\n",
    "\n",
    "If we set $f = \\left[\\hat{\\theta}(\\{x_i\\}) - \\theta \\right] \\sqrt{\\mathcal{L}}$, and $g = \\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta} \\sqrt{\\mathcal{L}}$, knowing that the integral of their product is unitary, we find\n",
    "$$\n",
    "\\left(\\int \\left[\\hat{\\theta}(\\{x_i\\}) - \\theta \\right]^2 \\mathcal{L} d^n x\\right)\\left(\\int \\left[\\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta}\\right]^2 \\mathcal{L} d^n x\\right)\\geq 1\\;.\n",
    "$$\n",
    "We can identify the first integral as $\\langle \\left(\\hat{\\theta}(\\{x_i\\}) - \\theta \\right)^2 \\rangle = V(\\hat{\\theta})$, therefore,\n",
    "$$\n",
    "V(\\hat{\\theta}) \\geq \\frac{1}{\\int \\left[\\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta}\\right]^2 \\mathcal{L} d^n x}\\;.\n",
    "$$\n",
    "\n",
    "The result we have found is of extreme importance! The variance of an unbiased estimator has a minimal value which depends only on our hypothesis about the distribution and its parameters. We have achieved this result without any data whatsoever. The denominator on the right side of the equation is called **Fisher Information**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d093f-32f5-48ec-86c8-206634a657b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lesson 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce89d6-c546-472d-9085-fecd9cd14444",
   "metadata": {},
   "source": [
    "## Variance of a Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e05acb1-ba0a-437f-98ca-7e59c2fef173",
   "metadata": {},
   "source": [
    "Say we have and identically independently distributed (i.i.d) set $\\{x_i\\}$, where all possible values comes from the same distribution $P(x)$. Then, we can say that $\\overline{x} = \\sum_i^N \\frac{x_i}{N}$ (arithmetic mean) is an estimator of the expected value , $\\langle x \\rangle = \\mu$, *i.e.*,\n",
    "$$\n",
    "\\hat{\\mu} = \\overline{x}\\;.\n",
    "$$\n",
    "\n",
    "We can now calculate the variance of the estimator, $V(\\hat{\\mu}) = \\langle \\hat{\\mu}^2 \\rangle - \\langle \\hat{\\mu} \\rangle^2$. First, we calculate $\\langle \\hat{\\mu}^2 \\rangle$:\n",
    "$$\n",
    "\\langle \\hat{\\mu}^2 \\rangle = \\bigg \\langle \\left(\\sum_i^N \\frac{x_i}{N} \\right)^2 \\bigg \\rangle = \\frac{1}{N}\\bigg \\langle \\sum_i^N  x_i^2 \\bigg \\rangle + \\frac{1}{N}\\bigg \\langle \\sum_{i \\ne j} x_i x_j \\bigg \\rangle\\;\\; \\bigg |\\;\\langle x_i^2\\rangle = V(x) + \\mu^2\\;,\\langle x_i x_j \\rangle = \\mu^2\\;;\n",
    "$$\n",
    "The sum over $i$ of a constant is $N$, and the sum over $i \\neq j$ of a constant is $N^2-N$, therefore the variance of our estimator will be\n",
    "$$\n",
    "V(\\hat{\\mu}) = \\frac{V(x)}{N}\\;.\n",
    "$$\n",
    "\n",
    "This is a significant value because it says the variance of our estimator is more narrow than the variance of our distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a8f555-7372-46e4-90f4-085d33f2dc1a",
   "metadata": {},
   "source": [
    "### Unbiased Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819528c6-406b-470b-89e8-ff581a107540",
   "metadata": {},
   "source": [
    "As we defined at the beginning of the course, the variance of a sample is\n",
    "$$\n",
    "\\hat{V}(x) = \\frac{1}{N} \\sum_{i=1}^N \\left(x_i - \\overline{x}\\right)^2\\;.\n",
    "$$\n",
    "Now, that we have knowledge of estimators such as $\\hat{\\mu} = \\overline{x}$, we can rewrite the expression above with this substitution, which gives us\n",
    "$$\n",
    "\\hat{V}(x) = \\frac{1}{N} \\sum_{i=1}^N \\left(x_i - \\hat{\\mu}\\right)^2\\ = \\frac{1}{N} \\sum_{i=1}^N \\left( x_i^2 - 2x_i\\hat{\\mu} + 2\\hat{\\mu}^2 \\right)\\;.\n",
    "$$\n",
    "\n",
    "To validate the bias of $\\hat{V}(x)$, its expected value should be equal to $\\hat{V}(x)$. Therefore,\n",
    "$$\n",
    "\\big \\langle \\hat{V}(x) \\big \\rangle = \\frac{1}{N} \\left[\\sum_{i=1}^N \\big \\langle \\left( x_i^2 - 2x_i\\hat{\\mu} + 2\\hat{\\mu}^2 \\right)\\big \\rangle\\right] = \\frac{1}{N} \\left[\\sum_{i=1}^N \\langle x_i^2 \\rangle - \\langle 2 \\hat{\\mu} \\sum_{i=1}^N x_i \\rangle + \\langle \\sum_{i=1}^N 2\\hat{\\mu}^2 \\rangle  \\right]\\;.\n",
    "$$\n",
    "Replacing some known values from the last section we get\n",
    "$$\n",
    "\\big \\langle \\hat{V}(x) \\big \\rangle = \\frac{N-1}{N} V(x)\\;.\n",
    "$$\n",
    "\n",
    "The previously defined variance is biased! Nevertheless, it is easy to eliminate such bias. We can simply redefine the variance as\n",
    "$$\n",
    "\\hat{V}(x) = \\frac{1}{N-1} \\sum_{i=1}^N \\left(x_i - \\hat{\\mu}\\right)^2\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef744816-2979-4eab-9a3d-50cffb41bef5",
   "metadata": {},
   "source": [
    "### Variance of the Variance Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048e6c8-df4e-4dca-816c-debc33400c69",
   "metadata": {},
   "source": [
    "!!HERE IT WOULD BE COOL TO PLOT AN INTERACTIVE GRAPH WITH MULTIPLE SAMPLES OF N VALUES AND ITS MEAN VALUES AND VARIANCE!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9097cdb-4640-4df0-963c-c093d179a988",
   "metadata": {},
   "source": [
    "It is also possible to calculate the variance of the variance estimator. It will be\n",
    "$$\n",
    "V(\\hat{V}) = \\big \\langle \\left(\\hat{V} - \\langle \\hat{V} \\rangle \\right)^2\\big \\rangle = \\bigg \\langle \\left( \\frac{1}{N-1} \\right)^2 \\left( \\sum_i (x_i - \\hat{\\mu}) \\right)^2 \\bigg \\rangle = \\frac{\\sigma^4}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8fe9f7-256d-45e9-99d0-19816a3cc71d",
   "metadata": {},
   "source": [
    "## Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db5aeb-dca4-4b37-b1ef-5a4c341ccc37",
   "metadata": {},
   "source": [
    "Suppose we have a accurate functional form of a distribution $P(x_i|\\theta)$, what should we expect from the likelihood $\\mathcal{L}(\\{x_i\\}|\\theta)$? Seems reasonable to think that a correct assumption will give us a maximal value of the likelihood function. \n",
    "\n",
    "**Maximum Likelihood Estimation** is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable [[Wiki]](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)\n",
    "\n",
    "We shall define a estimator $\\hat{\\theta} = Max\\; \\mathcal{L}(\\{x_i\\}|\\theta)$, simply calculated by\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta} \\bigg |_{\\hat{\\theta}} = 0\\;.\n",
    "$$\n",
    "\n",
    "Furthermore, given the scale of number of which we are working with it is common practice to work with $\\chi^2 = -2 \\ln \\mathcal{L}$. Then, the above estimator becomes $\\hat{\\theta} = Min\\; (-2 \\ln \\mathcal{L})$.\n",
    "\n",
    "In case we have more than one dimension, the maximization -or minimization- must be absolute, *i.e.*, the extreme value of every parameter. In other words, the gradient of the likelihood should be null. Otherwise, you will need to formulate a hypothesis for sets of parameters, since it is clearly difficult to have complete knowledge about the entirety of the space.\n",
    "\n",
    "Additionally, say we have a likelihood of a sample $x_i$, given a set of parameters $\\vec{\\theta}$ and $\\vec{\\psi}$. These parameter are inversible functions of each other, *i.e.*, $\\vec{\\theta}(\\vec{\\psi})$ and $\\vec{\\psi}(\\vec{\\theta})$. If we try to extremize $\\chi^2$, it is easy to see that a estimator is independent of the parametrization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f16bb-cd88-403c-9b47-6434fb1c9cf1",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb16691-ffa3-4601-8c83-1d3a661f5fa0",
   "metadata": {},
   "source": [
    "The likelihood of a gaussian distribution is\n",
    "$$\n",
    "\\mathcal{L}(\\{x_i\\}|\\mu_i\\{\\sigma_i\\}) = \\frac{\\prod e^{-\\frac{(x_i - \\mu_i)^2}{2\\sigma_i^2}}}{(\\sqrt{2 \\pi})^2\\prod \\sigma_i}\\;,\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\chi^2 = \\sum \\frac{(x_i - \\mu)^2}{\\sigma_i^2} + 2 \\sum \\ln \\sigma_i + \\frac{N}{2}\\ln 2\\pi\\;.\n",
    "$$\n",
    "\n",
    "Taking the derivative of $\\chi^2$ with respect to $\\mu$, calculating it in a random estimator $\\hat{\\mu}$, and setting it to zero to extremize, we get\n",
    "$$\n",
    "\\frac{\\partial \\chi^2}{\\partial \\mu} \\bigg |_{\\hat{\\mu}} = -2\\sum \\frac{(x_i - \\hat{\\mu})}{\\sigma_i^2}=0\\;.\n",
    "$$\n",
    "\n",
    "If we call $w_i = 1/\\sigma^2$ and $w = \\sum w_i$, the right equation becomes\n",
    "$$\n",
    "\\sum w_i x_i - \\hat{\\mu}\\sum w_i = 0\\;,\n",
    "$$\n",
    "which gives us the estimator with maximum likelihood,\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{w}\\sum_i w_i x_i\\;.\n",
    "$$\n",
    "This is the weighted average. Furthermore, if every $\\sigma_i = \\sigma$, then $w = N/\\sigma^2$, and\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{N}\\sum_i x_i\\;,\n",
    "$$\n",
    "which tells us the arithmetic mean also is an estimator with maximum likelihood.\n",
    "\n",
    "On top of that, if we want to find a the concurrent estimator with maximum likelihood for $\\sigma$, we take the derivative with respect to $\\sigma$ instead of $\\mu$. Doing so, we find\n",
    "$$\n",
    "\\frac{\\partial \\chi^2}{\\partial \\sigma} \\bigg |_{\\hat{\\sigma}} = -\\frac{2}{\\hat{\\sigma}^3}\\sum (x_i - \\hat{\\mu})^2 + \\frac{2N}{\\hat{\\sigma}} = 0\\;;\n",
    "$$\n",
    "$$\n",
    "\\hat{\\sigma}^2 =\\sum  \\frac{(x_i - \\hat{\\mu})^2}{N}\\;.\n",
    "$$\n",
    "We see that $\\hat{\\sigma}^2$ is the estimator of variance with maximum likelihood. Notice that this estimator is biased. The reason for that is the freedom of parametrization we previously discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e765657-4d56-495d-a3d5-6d8a1f048385",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lesson 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc798c2-3ba9-4ba0-bf2f-c19aa78c1b63",
   "metadata": {},
   "source": [
    "## Revisiting the Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce76618-9db6-4266-904c-b7ce13ed3cc4",
   "metadata": {},
   "source": [
    "In the following section, we want to show that a consistent estimator with maximum likelihood is asymptotically unbiased and tends to be the most efficient.\n",
    "\n",
    "Should we define random variable\n",
    "$$\n",
    "z = \\frac{1}{N}\\sum_{i=1}^N x_i\\;,\\;\\langle z \\rangle = \\frac{1}{N}\\sum_{i=1}^N \\mu_i = \\mu\n",
    "$$\n",
    "then $V(z)$ will be asymptotically null, *i.e*,\n",
    "$$\n",
    "V(z) = \\big \\langle \\left ( z - \\langle z \\rangle \\right)^2 \\big \\rangle = \\frac{1}{N^2} \\bigg \\langle \\left( \\sum^N (x_i - \\mu_i )\\right)^2 \\bigg \\rangle = \\frac{N}{N^2}\\overline{V(x_i)} = \\frac{\\overline{V(x_i)}}{N}\\;.\n",
    "$$\n",
    "\n",
    "When our random variable has a null variance for large samples, we say that it converges in probability.\n",
    "\n",
    "However, say we define $w = \\sqrt{N}(z - \\langle z \\rangle)$. The expected value is trivially zero, and the variance tends to a finite value as $N$ grows. These variables converge in distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515d1d4-79b2-4b2b-9b79-4d6182078610",
   "metadata": {},
   "source": [
    "A given parameter has a value $\\theta$ in the parametric space, a real value $\\theta_0$, and a estimator $\\hat{\\theta}$. If we define \n",
    "$$\n",
    "\\hat{\\theta} = Max _\\theta \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \\bigg |_\\hat{\\theta} = f(\\hat{\\theta}) = 0\\;.\n",
    "$$\n",
    "This estimator gives us a singular function for each sample, $\\hat{\\theta}(\\{x_i\\})$.\n",
    "\n",
    "Moreover, say this estimator is consistent, $f(\\hat{\\theta})$ can be expanded in a Taylor Series as\n",
    "$$\n",
    "f((\\hat{\\theta} - \\theta_0) + \\theta_0) = f(\\theta_0) + f'(\\theta_0)(\\hat{\\theta} - \\theta_0) + \\mathcal{O}(2) = 0\\;.\n",
    "$$\n",
    "The rightmost expression tells us that the actual value of a given parameter does not maximize the likelihood. This reflects oscillating values of $\\theta_0$, which is expected for different samples. Since this expression is valid for any function of $\\hat{\\theta}$, the following is also true:\n",
    "$$\n",
    "{\\chi^2}'(\\theta_0) + {\\chi^2}'' (\\theta_0)(\\hat{\\theta} - \\theta_0) = 0\\;.\n",
    "$$\n",
    "Where $\\chi^2 = -2 \\ln \\mathcal{L}$. \n",
    "\n",
    "If $\\mathcal{L} = \\prod P$, then, ${\\chi^2}' = \\sum_i {\\chi_i^2}'$, *i.e*, ${\\chi^2}'$ is a random variable. Calculating the expected value for $\\big \\langle {\\chi^2}' \\big \\rangle$ and $\\big \\langle {\\chi^2}'' \\big \\rangle$ we find\n",
    "$$\n",
    "\\big \\langle {\\chi^2}' \\big \\rangle = \\int \\frac{\\partial (-2\\ln \\mathcal{L})}{\\partial \\theta} \\mathcal{L} d^N x = -2 \\int \\frac{\\partial \\mathcal{L}}{\\partial \\theta} d^N x = -2 \\frac{\\partial}{\\partial \\theta} \\int \\mathcal{L} d^N x = 0\\;;\\\\\n",
    "\\frac{\\partial \\big \\langle {\\chi^2}' \\big \\rangle}{\\partial \\theta} = \\int \\left({\\chi^2}'' +  {\\chi^2}'\\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta}\\right)\\mathcal{L} d^N x = \\int \\left({\\chi^2}'' -  \\frac{({\\chi^2}')^2}{2}\\right)\\mathcal{L} d^N x = \\big \\langle {\\chi^2}'' \\big \\rangle - \\frac{\\big \\langle {\\chi^2}' \\big \\rangle}{2} = 0\n",
    "$$\n",
    "With $\\big \\langle {\\chi^2}' \\big \\rangle = 0$, the variance becomes\n",
    "$$\n",
    "V({\\chi^2} ') = \\big \\langle ({\\chi^2}')^2 \\big \\rangle = \\big \\langle (\\sum_i^N{\\chi_i^2}')^2 \\big \\rangle = N \\overline{\\big \\langle ({\\chi_i^2}')^2 \\big \\rangle}\\;\\; \\Bigg| \\;\\;\\;\\;\\big \\langle (\\sum_i^N{\\chi_i^2}')^2 \\big \\rangle = \\sum_i^N \\big \\langle ({\\chi_i^2}')^2 \\big \\rangle + Null\\left(\\sum_{i,j}^N \\big \\langle ({\\chi_i^2}')({\\chi_j^2}') \\big \\rangle \\right)\\;.\n",
    "$$\n",
    "As we can see, $V({\\chi^2} ') = N \\overline{\\big \\langle ({\\chi_i^2}')^2 \\big \\rangle}$. Since we want these variables to converge in distribution, we divide ${\\chi^2} '$ by $\\sqrt{N}$, then $V(W) = \\overline{\\big \\langle ({\\chi_i^2}')^2 \\big \\rangle}$ for $W = \\frac{{\\chi^2} '}{\\sqrt{N}}$.\n",
    "\n",
    "Calculating the Taylor Expansion for our new variable $W$, we can write:\n",
    "$$\n",
    "\\frac{{\\chi^2}'(\\theta_0)}{\\sqrt{N}} + \\frac{{\\chi^2}''(\\theta_0)}{N}\\left(\\sqrt{N} (\\hat{\\theta} - \\theta_0)\\right) = 0\\;.\n",
    "$$\n",
    "Now you must believe me, $\\frac{{\\chi^2}''(\\theta_0)}{N}$ tends to $\\big \\langle \\frac{{\\chi^2}''(\\theta_0)}{N} \\big \\rangle$ for large $N$. Knowing this, we can solve for ${N}\\left(\\sqrt{N} (\\hat{\\theta} - \\theta_0)\\right)$. Doing so, we find\n",
    "$$\n",
    "{N}\\left(\\sqrt{N} (\\hat{\\theta} - \\theta_0)\\right) = -\\frac{\\frac{{\\chi^2}'(\\theta_0)}{\\sqrt{N}}}{\\big \\langle \\frac{{\\chi^2}''(\\theta_0)}{N} \\big \\rangle}\\;.\n",
    "$$\n",
    "Now, calculating the variance of this last expression, \n",
    "$$\n",
    "V\\left(\\sqrt{N} (\\hat{\\theta} - \\theta_0)\\right) = 2\\frac{N}{\\big \\langle {\\chi^2}'' \\big \\rangle}\\;,\n",
    "$$we prove that estimators with maximum likelihood are not only unbiased but they are the most efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a353aca-a43f-4bb2-b210-23d91de5ba0d",
   "metadata": {},
   "source": [
    "# Lesson 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd447e-aabd-483f-8982-5d966254b24a",
   "metadata": {},
   "source": [
    "## Statistical Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3effb-697a-4400-bbe6-dc87ef571bdd",
   "metadata": {},
   "source": [
    "A hypothesis is an assumption over the frequency of something. As you can see we shall start with a vague description, from which we will build a firm definition. For instance, say the probability of a coin toss turning head is half. We suppose that after a number of tosses, half of them are heads and the other half are tails. In reality, we will not have the exact value half. For this reason, it is good to test our hypothesis to see how far we are from reality.\n",
    "\n",
    "We can base our studies on a thought experiment. Say a Geiger Counter is measuring cosmic rays and we want to know the distribution of values of the interval between measurements. Given a normalized distribution $f(x|\\theta) = \\theta e^{-\\theta x}| x\\in (0, \\infty)$, where $x$ is the time interval between measurements.\n",
    "\n",
    "The parameter $\\theta$ is fixed by two possible hypotheses. The null hypothesis, $H_0| \\theta = 1$, is acceptable if our data sample belongs in a interval $R$. On the other hand, we make another hypothesis $H_1| \\theta = 2$ if our data is not in said interval. Comparing our dominium with the interval $R$, we define the critical region, $C = D-R$. If $R = [0, 1]$, then $C = (1, \\infty)$. \n",
    "\n",
    "The first question we ask is, what is the probability of $H_0$ being true, and measuring the value of $x$ inside the critical region? Solving the integral\n",
    "$$\n",
    "\\alpha = \\int_1 ^{\\infty} f(x|1) dx = \\int_C \\mathcal{L} d^N x = e^{-1}\\;.\n",
    "$$\n",
    "A follow-up question is, what is the probability of $H_1$ being true, and measuring the value of $x$ outside the critical region? Solving the integral\n",
    "$$\n",
    "\\beta = \\int_0 ^{1} f(x|2) dx = \\int_R \\mathcal{L} d^N x = 1 - e^{-2}\\;.\n",
    "$$\n",
    "\n",
    "$\\alpha$ and $\\beta$ are called Type I and Type II Erros respectively. A Type I Error (false-positive) occurs if an investigator rejects a null hypothesis that is actually true in the population; a Type II Error (false-negative) occurs if the investigator fails to reject a null hypothesis that is actually false in the population. Although type I and type II errors can never be avoided entirely, the investigator can reduce their likelihood by increasing the sample size (the larger the sample, the lesser is the likelihood that it will differ substantially from the population). \n",
    "\n",
    "False-positive and false-negative results can also occur because of bias (observer, instrument, recall, etc.). (Errors due to bias, however, are not referred to as type I and type II errors.) Such errors are troublesome, since they may be difficult to detect and cannot usually be quantified¹. [[1]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996198/#:~:text=A%20type%20I%20error%20(false,actually%20false%20in%20the%20population.)\n",
    "\n",
    "We can build the following table:\n",
    "\n",
    "|                   | $H_0$ is True | $H_1$ is True   |\n",
    "| :---              |    :------:   |            ---: |\n",
    "| $x \\geq  1$       | Type I Error  | Correct         |\n",
    "| $ 1 \\geq x \\geq 0$| Correct       | Type II Error   |\n",
    "\n",
    "Our first principle shall be: Given $\\alpha$, the best test is the one that minimizes $\\beta$. We will be creating different tests by tweaking the acceptance interval. Say we want $\\alpha = 0.05$, then:\n",
    "$$\n",
    "0.05 = \\int_{x_0} ^{\\infty} f(x|1) dx = e^{-1x_0}\\;\\big|\\; \\beta = \\int_0^{x_0}f(x|2)dx = 1-e^{-2x_0}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5fa246-7644-4bb7-94f9-2aac40096c88",
   "metadata": {},
   "source": [
    "# Lesson 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f050b79-4a75-481b-934d-b78c9b478749",
   "metadata": {},
   "source": [
    "## $\\chi^2$-Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea5ed3a-ed72-4f4c-90f8-5287c10db41f",
   "metadata": {},
   "source": [
    "Given a set of measurements $\\mathcal{D} = \\{ \\vec{x}_i\\}_{i = (1,N)} \\in \\mathcal{M}$, the bining $\\mathcal{B}_\\mu$ - $\\;\\cup_{\\mu = 1}^\\mathcal{M} \\mathcal{B}_\\mu = \\mathcal{M}|\\; \\mathcal{B}_\\mu \\cap \\mathcal{B}_\\nu = \\{0\\}$ - gives rise to bins defined as:\n",
    "$$\n",
    "b_\\mu (\\mathcal{D}) = \\sum_i\n",
    "\\left\\{\n",
    "    \\begin{array}\\\\\n",
    "        1 & \\mbox{if } \\ \\vec{x}_i \\in \\mathcal{B}_\\mu \\\\\n",
    "        0 & \\mbox{if } \\ \\vec{x}_i \\notin \\mathcal{B}_\\mu\n",
    "    \\end{array}\n",
    "\\right.\\;.\n",
    "$$\n",
    "\n",
    "Knowing the likelihood, $\\mathcal{L}(\\vec{x}|\\vec{\\theta})$, we can tell the expected count of values inside each bin. The frequency of a certain bin is\n",
    "$$\n",
    "\\mathcal{f}_\\mu = N \\int_{\\mathcal{B}_\\mu} \\mathcal{L} d^Nx = N P_\\mu \\;.\n",
    "$$\n",
    "Moreover, the binomial probability of a $\\vec{b}$ is given by\n",
    "$$\n",
    "P_{binomial}(\\vec{b}) = \\prod P_\\mu^{b_\\mu}\\binom{N}{b_1 b_2 b_3 \\ldots b_N}\\;,\n",
    "$$\n",
    "and the Poisson probability of the dataset $\\mathcal{D}$ will be\n",
    "$$\n",
    "P_{Poisson}(\\mathcal{D}) = \\prod_\\mu e^{-f_\\mu} \\frac{f_\\mu^{b_\\mu}}{b_\\mu !} \\approx \\prod_\\mu \\frac{e^{\\frac{-(b_\\mu - f_\\mu)^2}{2 f_\\mu}}}{\\sqrt{2 \\pi f_\\mu}}\\;.\n",
    "$$\n",
    "Therefore, by taking the natural logarithmic of the likelihood function we find $\\chi^2_{Poisson}$\n",
    "$$\n",
    "\\chi^2_{Poisson}(\\mathcal{D}) = \\sum_{\\mu=1}^M \\frac{(b_\\mu - f_\\mu)^2}{f_\\mu}\\;.\n",
    "$$\n",
    "\n",
    "As we can see $\\chi^2$ is a random variable, and we will use it as a base to construct statistical hypothesis testing. We can also ask the probability of measuring $\\chi^2(\\mathcal{D}) = y$. This new probability is stated as\n",
    "$$\n",
    "P(\\chi^2(\\mathcal{D}) = y) = \\int \\delta(\\chi^2(\\vec{b}) - y) \\frac{e^{\\frac{-1}{2}\\sum\\frac{(b_\\mu - f_\\mu)^2}{f_\\mu}}}{(2 \\pi)^{\\frac{M}{2}} \\prod \\sqrt{f_\\mu}} d^M b\\;.\n",
    "$$\n",
    "We can make a variable change to $W_M = \\frac{b_\\mu - f_\\mu}{\\sqrt{f_\\mu}}$, therefore, the probability becomes\n",
    "$$\n",
    "P(\\chi^2(\\mathcal{D}) = y) = \\int \\delta(\\chi^2(\\vec{W}) - y) \\frac{e^{\\frac{-1}{2}\\sum W_M^2}}{(2 \\pi)^{\\frac{M}{2}}} d^M W\\;.\n",
    "$$\n",
    "It is easy to see that our only dependency on $W_N$ is in its module, therefore, it is possible to rewrite the above integral as a hypersphere of dimension $M$ with radii $|W|$ and its solid angle differential. Finally, the probability function will be:\n",
    "$$\n",
    "P(\\chi^2(\\mathcal{D}) = y) = \\frac{2^{\\frac{-M}{2}}}{\\Gamma\\left(\\frac{M}{2}\\right)} y^{M/2-2}e^{-\\frac{y}{2}}\\;.\n",
    "$$\n",
    "\n",
    "The above-stated probability gives us a distribution of a statistic $\\chi^2$ for different datasets $\\mathcal{D}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e4410-8363-40d1-ae9e-8cbb0362dba9",
   "metadata": {},
   "source": [
    "## Revisiting Statistical Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3fdf67-3c4f-46ef-a954-498b2d98a3d2",
   "metadata": {},
   "source": [
    "Let us rollback to the Geiger Counter example. Remember that the time between measurements is distributed by $b(x|\\theta) = \\theta e^{-x \\theta}$ and we have two hypothesis, $H_0| \\theta = 2$ and $H_1|\\theta = 1$. We define a crictical value of $x$ that if our measurent is greater than it, we take the null hypothesis -$H_0$- to be true. Else, the alternative hypothesis will be accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d35ffb9-ce37-41f7-83b8-51032d6081bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc25d3840a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def b(x, theta):\n",
    "    return theta * np.exp(-x*theta)\n",
    "\n",
    "x = np.linspace(0, 3, 50)\n",
    "\n",
    "plt.plot(x, b(x, 2), label = r\"$H_0 | \\theta = 2$\")\n",
    "plt.plot(x, b(x, 1), label = r\"$H_1 | \\theta = 1$\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Time between measurements')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e234b8c1-9238-4aee-989c-2d958a0201c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lesson 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2fb416-f84d-42d6-85d8-a0cb51ba1d3a",
   "metadata": {},
   "source": [
    "## Hypothesis Testing with the Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c273847-7e8f-4a0f-b874-e44bf45a075e",
   "metadata": {},
   "source": [
    "Given $\\mathcal{L}(\\mathcal{D}|\\theta)$, with $\\mathcal{D} = \\{x_i\\} \\in \\mathcal{M}$, our null hypothesis is\n",
    "$$\n",
    "H_0 = \\vec{\\theta} = \\vec{\\theta}_0\\;.\n",
    "$$\n",
    "\n",
    "We know that $\\mathcal{L}(\\mathcal{D}|\\vec{\\theta}_0)$ gives us the probability of a certain distribution given a particular model $\\vec{\\theta}_0$. Thinking of $\\mathcal{L}$ as a statiscs, we can calculate a distribution for said $\\theta$. Generally, we ask the question: What is the probability of a certain function of the likelihood being equal to a particular value $\\lambda$?\n",
    "$$\n",
    "P \\big( f(\\mathcal{L}(\\mathcal{D}|\\theta)) - \\lambda \\big) = \\int \\delta(f(\\mathcal{L}(\\mathcal{D}|\\theta)) - \\lambda)\\;\\mathcal{L}(\\mathcal{D}|\\theta)\\;d^Nx = P_{\\vec{\\theta}_0}(\\lambda)\n",
    "$$\n",
    "\n",
    "But how do we calculate $P_{\\vec{\\theta}_0}(\\lambda)$? We begin by simulating $\\mathcal{D}$ given a $\\mathcal{L}(\\mathcal{D}|\\vec{\\theta}_0)$. Secondly, we calculate $f_i = f(\\mathcal{L}(\\mathcal{D}|\\vec{\\theta}_0))$. We repeat these two steps $k$ times. Finally, we use $\\{f_i\\}_k$ to estimate $P_{\\vec{\\theta}_0}(\\lambda)$ via **KDE** or its bins.\n",
    "\n",
    "By doing so, we can now use actual data, and compare it with our values for $\\lambda$ -given said parameter $\\vec{\\theta}_0$- to decide whether our hypothesis is acceptable or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67068d89-c8fb-4d9f-a5bf-edd85ad34be5",
   "metadata": {},
   "source": [
    "## Discovering $\\vec{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd81a4b-e626-4b69-8e0e-c6a79407b317",
   "metadata": {},
   "source": [
    "We usually do not know the value for our parameters, therefore we will study the same function $f$ but with a caveat of having the parameter $\\vec{\\theta}_0$ free to vary with $\\mathcal{D}$, *i.e*,\n",
    "$$\n",
    "f \\big(\\mathcal{L}(\\mathcal{D}|\\vec{\\theta}(\\mathcal{D}))\\big)\\;.\n",
    "$$\n",
    "This mean we are now actualy test the functional form of our model. From this new stated freedom, we shall find $P_{\\vec{\\theta}_0}(\\lambda)$ by adding a new step on what we previously described. After simulating a sample $\\mathcal{D}$, we found a new $\\hat{\\vec{\\theta}}$ that maximize the likelihood. This new parameter will be used throughout the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c107092-bb3f-4013-9bf1-e474e8775836",
   "metadata": {},
   "source": [
    "# Lesson 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afdad2b-6cb5-4adf-be9f-537d4a01f11f",
   "metadata": {},
   "source": [
    "## Neyman-Pearson Lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e97b2e-bff2-464b-8f7e-77e8fd3e881b",
   "metadata": {},
   "source": [
    "We have studied the parameter $\\alpha$ is\n",
    "$$\n",
    "\\alpha = \\int_{C} \\mathcal{L} d^n x\\;,\n",
    "$$\n",
    "where $C$ is the critical region defined as\n",
    "$$\n",
    "C = \\bigg \\{\\mathcal{D}' \\;\\bigg|\\; f\\big(\\mathcal{L}(\\mathcal{D}' |\\theta_0)\\big) < \\bar{\\lambda}_{\\mathcal{D}}(\\alpha)\\;\\textbf{,}\\;\\; f\\big(\\mathcal{L}(\\mathcal{D}'|\\theta_0)\\big) > \\lambda_{\\mathcal{D}}(\\alpha)\\bigg \\}\\; .\n",
    "$$\n",
    "$\\alpha$ can also be calculated with \n",
    "$$\n",
    "\\int_{-\\infty}^{\\bar{\\lambda_0}} P_{\\theta_0}(\\lambda) d\\lambda + \\int_{\\lambda_0}^{\\infty}P_{\\theta_0}(\\lambda) d\\lambda = \\alpha\\;.\n",
    "$$\n",
    "\n",
    "The Neyman-Pearson Lemma states that for a given $\\alpha$, the best critical region is such that\n",
    "\n",
    "$$\n",
    "\\frac{\\mathcal{L}(\\mathcal{D}|\\vec{\\theta}_1)}{\\mathcal{L}(\\mathcal{D}|\\vec{\\theta}_0)} \\geq \\mathcal{K}\\; \\bigg | \\;\\;\\mathcal{D} \\in C\\;,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\mathcal{L}(\\mathcal{D}|\\vec{\\theta}_1)}{\\mathcal{L}(\\mathcal{D}|\\vec{\\theta}_0)} \\leq \\mathcal{K}\\; \\bigg | \\;\\;\\mathcal{D} \\in (\\mathcal{M} - C)\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a3b56-079b-4eb3-9baf-0eff4ff04c88",
   "metadata": {},
   "source": [
    "### Neyman-Pearson Lemma Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd9a59-7912-44ae-a2bb-994b53f08732",
   "metadata": {},
   "source": [
    "Exists a critical region $C'$, such that $\\int_{C'}\\mathcal{L}(\\mathcal{D}|\\vec{\\theta}_0) d^n x = \\alpha$. Say that the same is true to another region $C$. This means that\n",
    "$$\n",
    "\\int_{C'}\\mathcal{L}_0 d^n x =\\int_{C}\\mathcal{L}_0 d^n x\\;.\n",
    "$$\n",
    "\n",
    "Since $C$ and $C'$ coexists in $\\mathcal{M}$, we define $B = C \\cap C'$, $A = C - B$, and $A' = C' - B$. Therefore, it is possible to decontruct the two integrals into its regions, where both sides have the same integral in $B$, leaving us with\n",
    "$$\n",
    "\\int_{A'}\\mathcal{L}_0 d^n x =\\int_{A}\\mathcal{L}_0 d^n x\\;.\n",
    "$$\n",
    "\n",
    "We will now redefine Type I Error and Type II Error. Type II Error is the probability o $\\mathcal{D} \\in \\mathcal{M} - C ~ \\mathcal{M} - C'$, and $H_1$ is true. Furthermore, we may calculate $\\beta$ and $\\beta'$\n",
    "$$\n",
    "\\beta = 1 - \\int_{C}\\mathcal{L}_1 d^n x\\;\\textbf{,}\\;\\;\\beta' = 1 - \\int_{C'}\\mathcal{L}_1 d^n x\\;;\n",
    "$$\n",
    "$$\n",
    "\\beta' - \\beta = \\int_{C}\\mathcal{L}_1 d^n x - \\int_{C'}\\mathcal{L}_1 d^n x\\;;\n",
    "$$\n",
    "$$\n",
    "\\beta' - \\beta = \\int_{A}\\mathcal{L}_1 d^n x - \\int_{A'}\\mathcal{L}_1 d^n x\\;.\n",
    "$$\n",
    "\n",
    "Notice how $A \\subset C$ and $A' \\not\\subset C$. With this, we may rewrite the Neyman-Pearson Lemma.\n",
    "\n",
    "$$\n",
    "K\\mathcal{L}_0 \\leq \\mathcal{L}_1 \\;\\;\\forall\\; \\mathcal{D} \\in A\\;;\n",
    "$$\n",
    "$$\n",
    "K\\mathcal{L}_0 \\geq \\mathcal{L}_1 \\;\\;\\forall\\; \\mathcal{D} \\in A'\\;.\n",
    "$$\n",
    "\n",
    "Integrating on both sides\n",
    "$$\n",
    "K \\int_A \\mathcal{L}_0 d^n x \\leq \\int_A \\mathcal{L}_1 d^n x \\;\\;\\forall\\; \\mathcal{D} \\in A\\;;\n",
    "$$\n",
    "\n",
    "$$\n",
    "K \\int_{A'} \\mathcal{L}_0 d^n x \\geq \\int_{A'} \\mathcal{L}_1 d^n x \\;\\;\\forall\\; \\mathcal{D} \\in A'\\;.\n",
    "$$\n",
    "\n",
    "Subtracting both lines, we identify the expression for $\\beta' - \\beta$ on the right side of the inequation. Therefore,\n",
    "$$\n",
    "K \\Big (\\int_A \\mathcal{L}_0 d^n x - \\int_{A'} \\mathcal{L}_0 d^n x \\Big) \\leq \\beta' - \\beta\\;,\n",
    "$$\n",
    "where by the definition of $A$ and $A'$, we see that the left side is zero. Finnaly,\n",
    "\n",
    "$$\n",
    "\\beta' \\geq \\beta\\;\\textbf{.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fce2f6-cf7b-46da-9e32-5b86f0ef9736",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lesson 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfecc4f-3869-4d05-9326-2d778c3c4ec3",
   "metadata": {},
   "source": [
    "## Likelihood Ratio Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5321d064-fc18-4490-941a-6d4ed078f882",
   "metadata": {},
   "source": [
    "Say for instance that the null hypothesis admits that our parameters $\\vec{\\theta}$ belong to a subset of the parameter space,*i.e.*\n",
    "$$\n",
    "H_0 : \\vec{\\theta} \\in \\Theta_0 \\subset \\Theta\\;\\textbf{,}\\;\\;dim\\Theta = m\\;.\n",
    "$$\n",
    "The estimator of maximum likelihood conditioned to our null hypothesis is\n",
    "$$\n",
    "\\hat{\\vec{\\theta}}_0 = Arg\\; max_{\\vec{\\theta}\\in\\Theta_0} \\mathcal{L}(\\mathcal{D}|\\vec{\\theta})\\;,\n",
    "$$\n",
    "and the estimator of maximum likelihood without our null hypothesis is\n",
    "$$\n",
    "\\hat{\\vec{\\theta}} = Arg\\; max_{\\vec{\\theta}\\in\\Theta} \\mathcal{L}(\\mathcal{D}|\\vec{\\theta})\\;.\n",
    "$$\n",
    "With both estimators, we may rewrite the Neyman-Pearson Lemma as\n",
    "$$\n",
    "\\lambda = \\frac{\\mathcal{L}\\big(\\mathcal{D}\\;\\big|\\;\\hat{\\vec{\\theta}}_0(\\mathcal{D}|H_0, \\mathcal{I})\\big)}{\\mathcal{L}\\big(\\mathcal{D}\\;\\big|\\;\\hat{\\vec{\\theta}}(\\mathcal{D}|\\mathcal{I})\\big)}\\;,\n",
    "$$\n",
    "where our estimator $\\hat{\\vec{\\theta}}_0$ is a functional of the data, conditioned to our null hypothesis and every possible hypothesis, $\\mathcal{I}$. Whereas, $\\hat{\\vec{\\theta}}$ is also a functional of our data solely supposing $\\mathcal{I}$. \n",
    "\n",
    "Since our estimators are of maximum likelihood, it is clear that the ratio above will always be less or equal to $1$. The numerator is restricted to a particular subset of our space, while the denominator is free and always greater. This tests how maximal our null hypothesis is compared to an indifferent hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec55a2-c986-47a3-9f3d-ba2460b7fb27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
